# (PART) Metaprogramming {-}

# Introduction {#meta  .unnumbered}

```{r, include = FALSE}
source("common.R")
```

\index{non-standard evaluation} 

One of the most intriguing things about R is its capability for __metaprogramming__: the idea that code is itself data, and can be inspected and modified programmatically. This is powerful idea and deeply influences much R code. At a simple level this tooling allows you to write `library(purrr)` instead of `library("purrr")` and enables `plot(x, sin(x))` to label the axes with `x` and `sin(x)`. At a deeper level it allows `y ~ x1 * x2` to represent a model that predicts the value of `y` from `x1`, `x2` and all interactions. It allows `subset(df, x == y)` to be translated to `df[df$x == df$y, , drop = FALSE]`, and for `dplyr::filter(db, x == y)` to generate the SQL `WHERE x = y` when `db` is a remote database table.

Hard to get your head around at first; requires grappling with a new level of abstraction. Don't be surprised if you're frustrated or confused at first; this is a natural part of the process that happens to every one!

Metaprogramming is particularly important in R because it is well suited to facilitating interactive data analysis. There are two primary uses of metaprogramming that you have probably already seen:

* It makes it possible to trade precision for concision in functions like
  `subset()` and `dplyr::filter()` that make interactive data exploration
  faster in return for introducing some ambiguity.
  
* It makes it possible to build __domain specific languages__ (DSLs) that 
  tailor R's semantics to specific problem domains like visualisation or data
  manipulation.

Closely related to metaprogramming is __non-standard evalution__, or NSE for short. This a term that's commonly used to describe the behaviour of R functions, but there are two problems with the term that lead me to avoid it. Firstly, NSE is actually a property of an argument (or arguments) of a function, so talking about NSE functions is a little sloppy. Secondly, it's confusing to define something by what it is not (standard), so in this book I'll teach you more precise vocabulary. 

## Overview {-}

In the following chapters, you'll learn about the three big ideas that underpin metaprogramming:

* In __Expressions__, Chapter \@ref(expressions), you'll learn that all R code
  forms a tree. You'll learn how to visualise that tree, how the rules of R's
  grammar convert linear sequences of characters into a tree, and how to use
  recursive functions to work with code trees.
  
* In __Quasiquotation__, Chapter \@ref(quasiquotation), you'll learn to use 
  tools from rlang to capture ("quote") unevaluated function arguments. You'll
  also learn about quasiquotation, which provides a set of techniques for
  "unquoting" input that makes it possible to easily generate new trees from 
  code fragments.
  
* In __Evaluation__, Chapter \@ref(evaluation), you'll learn about the inverse 
  of quotation: evaluation. Here you'll learn about an important data structure,
  the __quosure__, which ensures correct evaluation by capturing both the code 
  to evaluate, and the environment in which to evaluate it. This chapter will 
  show you how to put  all the pieces together to understand how NSE in base 
  R works, and how to write your own functions that work like `subset()`.

* Finally, in __Translating R code__, Chapter \@ref(translation), you'll see 
  how to combine first-class environments, lexical scoping, and metaprogramming 
  to translate R code into other languages, namely HTML and LaTeX.

Each chapter follows the same basic structure. You'll get the lay of the land in introduction, then see a motivating example. Next you'll learn the big ideas using functions from rlang, and then we'll circle back to talk about how those ideas are expressed in base R. Each chapter finishes with a case study, using the ideas to solve a bigger problem.

## Big ideas

But before you dive into details, I wanted to give you an overview of the most important ideas that motivating what you'll learn:

* Code is data
* Code is a tree
* Code can generate code
* Code + environment -> results
* Can capture code + environment in a quosure


In this book I focus on the rlang package [@rlang] rather than the equivalent functions in base R. I do this to focus on the key ideas because implementation in base R has occurred piecemeal over the last twenty years. Once you have the basic ideas with rlang, I'll show you the equivalent with base R so you can use your knowledge to understand existing code. This approach seems backward to some, but I think it's easier to grasp the key theories in a clean programming environment, and then learn about the evolutionary quirks of base R.

```{r setup}
library(rlang)
```

### Code is data

The first big idea is that code is data: you can capture code and compute on it in the same way that you can capture numeric data and compute on it. To compute on code, you first need some way to capture it. You can capture your own code with `rlang::expr()`. You can think of it returning exactly what you pass in:

```{r}
expr(mean(x, na.rm = TRUE))
expr(10 + 100 + 1000)
```

More formally, we call captured code an __expression__. An expression isn't a single type of object, but is a collective term for any of four types (call, symbol, constant, or pairlist). In Chapter \@ref(expression), you'll learn about the details of the data types and how to inspect and manipulate them.

You use `expr()` to capture your own code. You need a slightly different tool to capture expressions supplied by someone else (i.e. in a function call) because `expr()` doesn't work:

```{r}
capture_it <- function(x) {
  expr(x)
}
capture_it(a + b + c)
```

Instead, you need to use a function specifically designed to capture user input supplied to a function argument: `enexpr()`. 

```{r}
capture_it <- function(x) {
  enexpr(x)
}
capture_it(a + b + c)
```

Once you have captured an expression, you can inspect and modify it. Complex expressions behave much like lists. That means you can modify them using `[[` and `$`:

```{r}
f <- expr(f(x = 1, y = 2))

# Add a new argument
f$z <- 3
f

# Or remove an argument:
f[[2]] <- NULL
f
```

Note that the first element of the call is the function to be called, which means the first argument is in the second position.

### Code is a tree

To do more complex manipulation with code, you need to fully understand its structure. Behind the scenes, almost every programming language represents code as a tree, often called the __abstract syntax tree__, or AST for short. R is unusual in that you can actually inspect and manipulate this tree.

A very convenient tool for understanding the tree-like structure is `lobstr::ast()`. Given some code, will display the underlying tree structure. Function calls form the branches of the tree, and are shown by rectangles. The leaves of the tree are symbols (like `a`) and constants (like "b").

```{r}
lobstr::ast(f(a, "b"))
```

Nested function calls become branches in the tree:

```{r}
lobstr::ast(f1(f2(a, b), f3(1, 2)))
```

Because all function forms in can be written in prefix form (Section \@ref(prefix-form)), every R expression can be displayed in this way:

```{r}
lobstr::ast(1 + 2 * 3)
```

Displaying the code tree in this way provides useful tools for exploring R's grammar, the topic of Section \ref(grammar).

### Code can generate code

As well as seeing the tree from code typed by a human, you can also use code to create new trees. There are two main tools. Firstly, you can use `rlang::call2()`, which constructs a function call from its components: the function to call, and the arguments to call it with.

```{r}
call2("f", 1, 2, 3)
call2("+", 1, call2("*", 2, 3))
```

This is often convenient to program with, but is a bit clunkly for interactive use. An alternative technique is to build complex code trees by inserting simpler code trees into a template. `expr()` and `enexpr()` have built-in support for this idea via `!!`, the __unquote operator__ (pronounced bang-bang). 

The exact details are covered in Chapter \@ref(quotation), but basically `!!x` inserts the code tree stored in `x`. This makes it easy to correctly build up more complex trees from simple fragments:

```{r}
xy <- expr(x + y)
yz <- expr(y + z)

expr(!!xy / !!yz)
```

Notice that the output preserves the operator precedence so we get `(x + y) / (y + z)` not `x + y / y + z` (i.e. `x + (y / y) + z)`. This is important to note, particularly if you've been thinking "wouldn't this be easier to do by pasting strings together?

This pattern gets even more useful when you wrap it up into a function, first using `enexpr()` to capture the user's expression, then `expr()` and `!!` to create an new expression using a template. You'll see this pattern alot when wrapping tidyverse functions. The code below generates the code that you could evaluate to compute the coefficient of variation (or cv for short).

```{r}
cv <- function(var) {
  var <- enexpr(var)
  expr(mean(!!var) / sd(!!var))
}

cv(x)
cv(x + y)
```

Importantly, this works even when given weird variable names:

```{r}
cv(`)`)
```

This is another good reason to avoid pasting strings together when generating code. You might think this is an esoteric concern, but not worrying about this problem when generating SQL code from web applications lead to SQL injection attacks that have collectively cost billions of dollars. 

These techniques become even more powerful when combined it with functional programming. You'll explore these ideas in detail in Section \@ref(quasi-case-studies) but the teaser belows shows how you might generate a complex model specification from simple inputs.

```{r}
library(purrr)

poly <- function(n) {
  i <- as.double(seq(2, n))
  xs <- c(1, expr(x), map(i, function(i) expr(I(x^!!i))))
  terms <- reduce(xs, call2, .fn = "+")
  expr(y ~ !!terms)
}
poly(5)
```

### Evaluation runs code in an environment

Inspecting and modifying code gives you one set of powerful tools. You get another set of powerful tools when you run, or __evaluate__ code. Evaluating code also requires an environment which tells R what the symbols found in the leaves of the tree mean.

The primary tool for evaluating expressions is `base::eval()`, which takes an expression and an environment:

```{r}
eval(expr(x + y), env(x = 1, y = 10))
eval(expr(x + y), env(x = 2, y = 100))
```

If you omit the environment, it will use the current environment. Here that's the global environment:

```{r}
x <- 10
y <- 100
eval(expr(x + y))
```

There are two main ways that you can adapt the environment for fun and profit. You can temporarily override functions to implement a DSL, or add a data mask in order to refer to variables in a data frame as if they are variables in an environment.

### You can override functions to make a DSL

An even more powerful technique is to override functions. This is a big idea that we'll come back to in Chapter \@ref(translating), but I wanted to show a small example here. The code math example below evalutes code in a special environment where the basic algebraic operators have been overridden to work instead with strings:

```{r}
string_math <- function(x) {
  e <- env(
    caller_env(),
    `+` = function(x, y) paste0(x, y),
    `*` = function(x, y) strrep(x, y),
    `-` = function(x, y) sub(paste0(y, "$"), "", x),
    `/` = function(x, y) substr(x, 1, nchar(x) / y)
  )

  eval(enexpr(x), e)
}

name <- "Hadley"
string_math("Hi" - "i" + "ello " + name)
string_math("x-" * 3 + "y")
```

More extensive use of this idea likes to tools like dplyr. One of the headline features of dplyr[^inspiration] is that you can write R code that is automatically translated into SQL:

```{r}
library(dplyr)

con <- DBI::dbConnect(RSQLite::SQLite(), filename = ":memory:")
mtcars_db <- copy_to(con, mtcars)

mtcars_db %>%
  filter(cyl > 2) %>%
  select(mpg:hp) %>%
  head(10) %>%
  show_query()

DBI::dbDisconnect(con)
```

[^inspiration]: Base R functions like `subset()` and `transform()` inspired the development of dplyr.

If you're interested in learning more, I highly recommend  _Domain Specific Languages_ [@dsls]. It discusses many options for creating a DSL and provides many examples of different languages.

### Blur the line between data frames and environments with data masks

A more immediately practical application is to modify evaluation to look for variables in a data frame instead of an environment. This idea powers the base `subset()` and `transform()` functions, as well as many tidyverse functions like `ggplot2::aes()` and `dplyr::mutate()`. It's possible to use `eval()` for this, but there are a few potential pitfalls, so we'll use `rlang::eval_tidy()` instead:

```{r}
df <- data.frame(x = 1:5, y = sample(5))
eval_tidy(expr(x + y), df)
```

Here `df` is called a __data mask__ because it masks objects in the active environment. It comes with a cost because `eval_tidy(expr(x + y), df)` is ambiguous: you need to know the variables of `df` in order to predict whether it's equivalent to `df$x + df$y` or `x + y` or some other combination. This can be important so `eval_tidy()` provides the `.data` and `.env` pronouns which allow you to make it clear which you refer to:

```{r}
x <- 100
eval_tidy(expr(.data$x + .env$y), df)
```

This allows us to create tools like `with()`, where we can evaluate code 

```{r}
with2 <- function(df, expr) {
  eval_tidy(enexpr(expr), df)
}

with2(df, x + y)
```

### Quosures

When you, the developwr, are generating code, you know exactly where it should be evaluated. But how do you do the same for user code, which could be generated in any number of environments?

Another more subtle problem occurs when we mix user code with our code. I'm goign to modify `with2()` to make the problem a little more obvious. The problem occurs without this modification but it's a bit sublter and creates error messages that are confusing to understand.

```{r}
with2 <- function(df, expr) {
  a <- 1000
  eval_tidy(enexpr(expr), df)
}
```

If we attempt to use `with2()` with some variables from the data frame and some from the local environment we get a confusing result:

```{r}
df <- data.frame(x = 1:3)
a <- 10
with2(df, x + a)
```

That's because we have lost the environment associated with `expr`, and it is evaluted inside the function where `a` is 1000.

Fortunately rlang provides another tool to deal with this problem: __quosures__ which bundle code and an environment. `eval_tidy()` knows how to work with quosures so all we need to do is switch out `enexpr()` for `enquo()` 

```{r}
with2 <- function(df, expr) {
  a <- 1000
  eval_tidy(enquo(expr), df)
}

with2(df, x + a)
```

Whenever you use a data mask, you must always use `enquo()` instead of `enexpr()`. This is the topic of Chapter \@ref()
